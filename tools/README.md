# KCS Tools

This directory contains command-line tools and scripts for kernel indexing,
chunk processing, and system administration.

## Core Tools

### Kernel Indexing

#### `index_kernel.sh` - Main Kernel Indexing Tool

The primary tool for indexing Linux kernel source code. Produces chunked
JSON output (legacy single-file mode has been removed).

**Basic Usage:**

```bash
# Index entire kernel (chunked output)
./tools/index_kernel.sh \
  --chunk-size 50MB \
  --parallel-chunks 4 \
  --output-dir /tmp/kcs-chunks \
  ~/src/linux
```

**Chunking Options:**

```bash
# Standard chunked indexing with 50MB chunks
./tools/index_kernel.sh \
  --chunk-size 50MB \
  --parallel-chunks 4 \
  --output-dir /tmp/kcs-chunks \
  ~/src/linux

# Custom chunk size and parallelism
./tools/index_kernel.sh \
  --chunk-size 25MB \
  --parallel-chunks 8 \
  --output-dir /var/cache/kcs \
  ~/src/linux

# Incremental update with existing chunks
./tools/index_kernel.sh \
  --incremental \
  --manifest /tmp/kcs-chunks/manifest.json \
  ~/src/linux

# Subsystem-only indexing (faster for testing)
./tools/index_kernel.sh \
  --subsystem fs/ext4 \
  --chunk-size 10MB \
  --output-dir /tmp/ext4-chunks \
  ~/src/linux

# With specific kernel configuration
./tools/index_kernel.sh \
  --config arm64:defconfig \
  --chunk-size 50MB \
  --parallel-chunks 2 \
  --output-dir /tmp/arm64-chunks \
  ~/src/linux
```

**Advanced Chunking Workflows:**

1. **Large Kernel (>1GB) Processing:**

   ```bash
   # Phase 1: Generate chunks with moderate parallelism
   ./tools/index_kernel.sh \
     --chunk-size 50MB \
     --parallel-chunks 4 \
     --output-dir /var/cache/kcs/chunks \
     --include-calls \
     ~/src/linux-stable

   # Phase 2: Process chunks into database with high parallelism
   ./tools/process_chunks.py \
     --manifest /var/cache/kcs/chunks/manifest.json \
     --parallel 8 \
     --batch-size 10
   ```

2. **Memory-Constrained Environments:**

   ```bash
   # Use smaller chunks and limited parallelism
   ./tools/index_kernel.sh \
     --chunk-size 25MB \
     --parallel-chunks 2 \
     --output-dir /tmp/small-chunks \
     ~/src/linux

   # Process with conservative memory settings
   ./tools/process_chunks.py \
     --manifest /tmp/small-chunks/manifest.json \
     --parallel 2 \
     --batch-size 5
   ```

3. **Development and Testing:**

   ```bash
   # Quick indexing for testing (single subsystem)
   ./tools/index_kernel.sh \
     --subsystem drivers/net/ethernet \
     --chunk-size 10MB \
     --output-dir /tmp/test-chunks \
     ~/src/linux

   # Process test chunks
   ./tools/process_chunks.py \
     --manifest /tmp/test-chunks/manifest.json \
     --parallel 1
   ```

#### `process_chunks.py` - Chunk Database Processing

Processes JSON chunks generated by `index_kernel.sh` into the PostgreSQL database.

**Basic Usage:**

```bash
# Process all chunks from manifest
./tools/process_chunks.py \
  --manifest /tmp/kcs-chunks/manifest.json \
  --parallel 4

# Resume processing after failure
./tools/process_chunks.py \
  --manifest /tmp/kcs-chunks/manifest.json \
  --resume

# Process with custom batch size
./tools/process_chunks.py \
  --manifest /tmp/kcs-chunks/manifest.json \
  --parallel 8 \
  --batch-size 20

# Process specific chunks only
./tools/process_chunks.py \
  --manifest /tmp/kcs-chunks/manifest.json \
  --chunk-ids kernel_001,kernel_002,kernel_015
```

**Chunking Workflow Examples:**

1. **Complete Workflow - Development Environment:**

   ```bash
   # Setup output directory
   mkdir -p /tmp/kcs-dev

   # Index kernel with chunking
   ./tools/index_kernel.sh \
     --chunk-size 25MB \
     --parallel-chunks 2 \
     --output-dir /tmp/kcs-dev \
     --subsystem fs/ext4 \
     ~/src/linux

   # Verify chunks were created
   ls -lh /tmp/kcs-dev/
   cat /tmp/kcs-dev/manifest.json | jq '.total_chunks'

   # Process into database
   ./tools/process_chunks.py \
     --manifest /tmp/kcs-dev/manifest.json \
     --parallel 2

   # Verify database content
   psql -d kcs -c "SELECT COUNT(*) FROM symbol;"
   ```

2. **Complete Workflow - Production Environment:**

   ```bash
   # Setup persistent storage
   sudo mkdir -p /var/cache/kcs/chunks
   sudo chown $USER:$USER /var/cache/kcs/chunks

   # Full kernel indexing with optimal settings
   ./tools/index_kernel.sh \
     --chunk-size 50MB \
     --parallel-chunks 6 \
     --output-dir /var/cache/kcs/chunks \
     --include-calls \
     --config x86_64:defconfig \
     ~/src/linux-stable

   # Monitor chunk generation progress
   watch -n 5 'ls /var/cache/kcs/chunks/*.json | wc -l'

   # Process chunks with production settings
   ./tools/process_chunks.py \
     --manifest /var/cache/kcs/chunks/manifest.json \
     --parallel 10 \
     --batch-size 25 \
     --log-level INFO

   # Monitor processing progress
   psql -d kcs -c "
     SELECT status, COUNT(*)
     FROM chunk_processing_status
     GROUP BY status;"
   ```

3. **Incremental Update Workflow:**

   ```bash
   # Initial indexing
   ./tools/index_kernel.sh \
     --chunk-size 50MB \
     --output-dir /var/cache/kcs/v6.7 \
     ~/src/linux-6.7

   ./tools/process_chunks.py \
     --manifest /var/cache/kcs/v6.7/manifest.json \
     --parallel 8

   # Later: incremental update for new kernel version
   ./tools/index_kernel.sh \
     --incremental \
     --manifest /var/cache/kcs/v6.7/manifest.json \
     --output-dir /var/cache/kcs/v6.8 \
     ~/src/linux-6.8

   ./tools/process_chunks.py \
     --manifest /var/cache/kcs/v6.8/manifest.json \
     --parallel 8 \
     --resume
   ```

### Entry Point Extraction

#### `extract_entrypoints_streaming.py` - Streaming Entry Point Detection

Extracts kernel entry points (syscalls, ioctls, file_ops, sysfs) using streaming output.

**Usage:**

```bash
# Extract all entry points
./tools/extract_entrypoints_streaming.py ~/src/linux

# Extract specific types
./tools/extract_entrypoints_streaming.py \
  --entry-types syscall,file_ops \
  ~/src/linux

# Extract with subsystem filter
./tools/extract_entrypoints_streaming.py \
  --subsystem fs/ext4 \
  ~/src/linux

# Combine with chunking workflow
./tools/extract_entrypoints_streaming.py ~/src/linux | \
  head -20  # Preview first 20 entry points
```

#### `extract_syscalls_fast.sh` - Fast Syscall Extraction

Quick syscall enumeration for testing and validation.

**Usage:**

```bash
# Extract syscalls only
./tools/extract_syscalls_fast.sh ~/src/linux
```

## System Administration

### `quick-setup.sh` - Development Environment Setup

Sets up a complete KCS development environment with dependencies, database, and tools.

**Usage:**

```bash
# Full setup
./tools/quick-setup.sh

# Setup with custom database name
DATABASE_NAME=kcs_dev ./tools/quick-setup.sh
```

### `run_system_test.py` - System Integration Testing

Runs comprehensive system tests including chunking workflows.

**Usage:**

```bash
# Run all system tests
./tools/run_system_test.py

# Run chunking-specific tests
./tools/run_system_test.py --test-suite chunking

# Test with specific chunk configuration
./tools/run_system_test.py \
  --chunk-size 25MB \
  --test-kernel ~/src/linux-test
```

## Performance and Optimization

### `performance_optimization.py` - Performance Analysis

Analyzes and optimizes KCS performance, including chunking efficiency.

**Usage:**

```bash
# Analyze chunking performance
./tools/performance_optimization.py \
  --analyze-chunks /var/cache/kcs/chunks/

# Benchmark chunk processing
./tools/performance_optimization.py \
  --benchmark-processing \
  --manifest /tmp/kcs-chunks/manifest.json
```

### `generate_docs.py` - Documentation Generation

Generates API documentation and performance reports.

**Usage:**

```bash
# Generate API docs
./tools/generate_docs.py --api

# Generate chunk processing reports
./tools/generate_docs.py \
  --chunk-report /var/cache/kcs/chunks/
```

## Chunk Management Tips

### Optimal Chunk Sizes

- **50MB**: Constitutional maximum, best for production
- **25MB**: Good balance for development systems
- **10MB**: Testing and memory-constrained environments

### Parallelism Guidelines

- **CPU cores**: Use 0.5-1x CPU cores for chunk generation
- **Memory**: Ensure 200MB+ RAM per parallel chunk
- **I/O**: Consider disk speed for chunk write performance

### Storage Requirements

- **Chunks**: ~20% larger than original parsed data due to JSON overhead
- **Database**: ~30% of chunk size after processing
- **Temporary**: 2x chunk size during processing

### Monitoring and Troubleshooting

1. **Check chunk generation progress:**

   ```bash
   tail -f /tmp/kcs-chunks/*.log
   ls /tmp/kcs-chunks/*.json | wc -l
   ```

2. **Monitor database processing:**

   ```bash
   psql -d kcs -c "SELECT status, COUNT(*) FROM chunk_processing_status GROUP BY status;"
   ```

3. **Verify chunk integrity:**

   ```bash
   ./tools/process_chunks.py \
     --manifest /tmp/kcs-chunks/manifest.json \
     --verify-only
   ```

## Environment Variables

- `DATABASE_URL`: PostgreSQL connection string
- `KCS_CHUNK_SIZE`: Default chunk size (e.g., "50MB")
- `KCS_PARALLEL_CHUNKS`: Default parallelism level
- `KCS_OUTPUT_DIR`: Default output directory for chunks
- `RUST_LOG`: Rust logging level (debug, info, warn, error)
- `PYTHONPATH`: Ensure KCS Python modules are found

## Docker Integration

### Using with Docker Compose

```bash
# Start services with chunking support
docker compose -f tools/docker/docker-compose.yml up

# Run indexing in container
docker compose exec kcs ./tools/index_kernel.sh \
  --chunk-size 50MB \
  --output-dir /var/cache/kcs \
  /opt/kernel

# Process chunks in container
docker compose exec kcs ./tools/process_chunks.py \
  --manifest /var/cache/kcs/manifest.json
```

## Dependencies

- **Rust**: kcs-parser, kcs-serializer, kcs-extractor
- **Python 3.10+**: asyncio, aiofiles, asyncpg, structlog
- **PostgreSQL 14+**: with pgvector extension
- **System**: tree-sitter, libclang (optional)

## See Also

- [CLAUDE.md](../CLAUDE.md) - Development workflows and conventions
- [specs/006-multi-file-json/](../specs/006-multi-file-json/) - Chunking implementation details
- [tests/performance/](../tests/performance/) - Performance benchmarks and tests
